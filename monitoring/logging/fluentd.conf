# Fluentd configuration for Agent Mesh log aggregation

# Input sources
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Docker container logs
<source>
  @type tail
  @id in_tail_container_logs
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag raw.kubernetes.*
  read_from_head true
  <parse>
    @type multi_format
    <pattern>
      format json
      time_key time
      time_format %Y-%m-%dT%H:%M:%S.%NZ
    </pattern>
    <pattern>
      format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
      time_format %Y-%m-%dT%H:%M:%S.%N%:z
    </pattern>
  </parse>
</source>

# Filter and parse Agent Mesh specific logs
<filter raw.kubernetes.**>
  @type kubernetes_metadata
  @id filter_kube_metadata
  kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV['KUBERNETES_SERVICE_HOST'] + ':' + ENV['KUBERNETES_SERVICE_PORT'] + '/api'}"
  verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
  ca_file "#{ENV['KUBERNETES_CA_FILE']}"
  skip_labels "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
  skip_container_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
  skip_namespace_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA'] || 'false'}"
</filter>

# Parse structured logs from Agent Mesh
<filter kubernetes.**agent-mesh**>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  emit_invalid_record_to_error false
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</filter>

# Add environment and service information
<filter kubernetes.**agent-mesh**>
  @type record_transformer
  <record>
    service_name agent-mesh
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
    cluster_name "#{ENV['CLUSTER_NAME'] || 'agent-mesh-cluster'}"
    log_level ${record["level"] || "INFO"}
  </record>
</filter>

# Security log filtering
<filter kubernetes.**agent-mesh**>
  @type grep
  <regexp>
    key log
    pattern /SECURITY|AUTH|CRYPTO|CERTIFICATE/
  </regexp>
  tag security.logs
</filter>

# Performance log filtering  
<filter kubernetes.**agent-mesh**>
  @type grep
  <regexp>
    key log
    pattern /PERFORMANCE|LATENCY|THROUGHPUT|BENCHMARK/
  </regexp>
  tag performance.logs
</filter>

# Error log filtering
<filter kubernetes.**agent-mesh**>
  @type grep
  <regexp>
    key level
    pattern /ERROR|CRITICAL|FATAL/
  </regexp>
  tag error.logs
</filter>

# Consensus-specific logs
<filter kubernetes.**agent-mesh**>
  @type grep
  <regexp>
    key log
    pattern /CONSENSUS|VOTE|PROPOSAL|BYZANTINE/
  </regexp>
  tag consensus.logs
</filter>

# Federated learning logs
<filter kubernetes.**agent-mesh**>
  @type grep
  <regexp>
    key log
    pattern /FEDERATED|TRAINING|AGGREGATION|MODEL/
  </regexp>
  tag federated.logs
</filter>

# P2P network logs
<filter kubernetes.**agent-mesh**>
  @type grep
  <regexp>
    key log
    pattern /P2P|PEER|NETWORK|CONNECTION/
  </regexp>
  tag network.logs
</filter>

# Output configurations

# Send all logs to Elasticsearch
<match kubernetes.**agent-mesh**>
  @type elasticsearch
  @id out_es
  @log_level info
  include_tag_key true
  host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['FLUENT_ELASTICSEARCH_PORT'] || '9200'}"
  logstash_format true
  logstash_prefix agent-mesh
  logstash_dateformat %Y.%m.%d
  include_timestamp true
  type_name _doc
  template_name agent-mesh
  template_file /fluentd/etc/elasticsearch-template.json
  template_overwrite true
  
  # Buffer configuration
  <buffer>
    @type file
    path /var/log/fluentd-buffers/kubernetes.system.buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>

# Send security logs to dedicated index
<match security.logs>
  @type elasticsearch
  @id out_es_security
  host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['FLUENT_ELASTICSEARCH_PORT'] || '9200'}"
  logstash_format true
  logstash_prefix agent-mesh-security
  logstash_dateformat %Y.%m.%d
  type_name _doc
  
  <buffer>
    @type file
    path /var/log/fluentd-buffers/security.buffer
    flush_mode interval
    flush_interval 10s
    retry_forever
    chunk_limit_size 1M
  </buffer>
</match>

# Send error logs to dedicated index and alerting
<match error.logs>
  @type copy
  <store>
    @type elasticsearch
    host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'elasticsearch'}"
    port "#{ENV['FLUENT_ELASTICSEARCH_PORT'] || '9200'}"
    logstash_format true
    logstash_prefix agent-mesh-errors
    logstash_dateformat %Y.%m.%d
    type_name _doc
  </store>
  <store>
    @type http
    endpoint http://alertmanager:9093/api/v1/alerts
    http_method post
    serializer json
    headers {"Content-Type":"application/json"}
    <format>
      @type json
    </format>
    <buffer>
      flush_interval 30s
      chunk_limit_size 1M
    </buffer>
  </store>
</match>

# Performance logs to time-series database
<match performance.logs>
  @type prometheus
  <metric>
    name agent_mesh_log_performance_counter
    type counter
    desc Agent Mesh performance log counter
    <labels>
      service_name ${service_name}
      environment ${environment}
      level ${log_level}
    </labels>
  </metric>
</match>

# Send to S3 for long-term archival
<match **>
  @type s3
  @id out_s3
  s3_bucket "#{ENV['S3_BUCKET_NAME'] || 'agent-mesh-logs'}"
  s3_region "#{ENV['S3_REGION'] || 'us-west-2'}"
  path logs/
  time_slice_format %Y/%m/%d/%H
  
  # Buffer configuration for S3
  <buffer time>
    @type file
    path /var/log/fluentd-buffers/s3
    timekey 3600
    timekey_wait 10m
    timekey_use_utc true
    chunk_limit_size 256m
  </buffer>
  
  <format>
    @type json
  </format>
</match>

# Prometheus metrics output
<match fluent.**>
  @type prometheus
  <metric>
    name fluentd_output_status_num_records_total
    type counter
    desc The total number of outgoing records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
</match>