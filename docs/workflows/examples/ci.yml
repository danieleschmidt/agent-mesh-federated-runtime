# Continuous Integration Workflow for Agent Mesh Federated Runtime
# This workflow runs on pull requests and pushes to main/develop branches
# 
# MANUAL SETUP REQUIRED:
# 1. Copy this file to .github/workflows/ci.yml
# 2. Configure required secrets in repository settings
# 3. Set up branch protection rules
# 4. Review and customize for your specific needs

name: Continuous Integration

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - '.gitignore'
      - 'LICENSE'
  pull_request:
    branches: [ main, develop ]
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - '.gitignore'
      - 'LICENSE'
  workflow_dispatch:
    inputs:
      skip_tests:
        description: 'Skip test execution (for debugging)'
        required: false
        default: false
        type: boolean

# Global environment variables
env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  DOCKER_BUILDKIT: '1'
  REGISTRY: ghcr.io
  IMAGE_NAME: agent-mesh-federated-runtime
  COVERAGE_THRESHOLD: '85'

# Ensure only one CI run per PR/branch
concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # =============================================================================
  # CODE QUALITY AND LINTING
  # =============================================================================
  code-quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Install Node.js dependencies
      run: |
        npm install
        if [ -d "src/web/dashboard" ]; then
          cd src/web/dashboard && npm install
        fi
    
    - name: Cache pre-commit
      uses: actions/cache@v4
      with:
        path: ~/.cache/pre-commit
        key: pre-commit-${{ hashFiles('.pre-commit-config.yaml') }}
    
    - name: Run pre-commit hooks
      run: |
        pre-commit install
        pre-commit run --all-files --show-diff-on-failure
    
    - name: Python linting (flake8)
      run: |
        flake8 src tests --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src tests --count --exit-zero --max-complexity=12 --max-line-length=88 --statistics
    
    - name: Python type checking (mypy)
      run: |
        mypy src --ignore-missing-imports --show-error-codes
    
    - name: JavaScript/TypeScript linting
      if: ${{ hashFiles('src/web/dashboard/package.json') != '' }}
      run: |
        cd src/web/dashboard
        npm run lint
    
    - name: Check for secrets
      run: |
        if [ -f ".secrets.baseline" ]; then
          detect-secrets scan --baseline .secrets.baseline
        else
          detect-secrets scan --all-files
        fi
    
    - name: Upload lint results
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: lint-results
        path: |
          lint-report.txt
          mypy-report.txt
          .secrets.baseline

  # =============================================================================
  # SECURITY SCANNING
  # =============================================================================
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    timeout-minutes: 20
    permissions:
      security-events: write  # For uploading SARIF results
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit[toml] safety
    
    - name: Run Bandit security scan
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ -f txt
    
    - name: Run Safety dependency scan
      run: |
        safety check --json --output safety-report.json || true
        safety check
    
    - name: Upload security scan results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: security-scan-results
        path: |
          bandit-report.json
          safety-report.json
    
    # CodeQL Analysis for additional security scanning
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: python, javascript
        queries: security-extended,security-and-quality
    
    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
      with:
        category: "/language:${{matrix.language}}"

  # =============================================================================
  # UNIT TESTS
  # =============================================================================
  unit-tests:
    name: Unit Tests
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    if: ${{ !inputs.skip_tests }}
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.9', '3.10', '3.11', '3.12']
        include:
          - os: ubuntu-latest
            python-version: '3.11'
            coverage: true
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ \
          --verbose \
          --tb=short \
          --junitxml=test-results-unit.xml \
          ${{ matrix.coverage && '--cov=src/agent_mesh --cov-report=xml --cov-report=html' || '' }}
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-unit-${{ matrix.os }}-py${{ matrix.python-version }}
        path: |
          test-results-unit.xml
          htmlcov/
          coverage.xml
    
    - name: Upload coverage to Codecov
      if: matrix.coverage
      uses: codecov/codecov-action@v4
      with:
        files: ./coverage.xml
        flags: unit-tests
        name: unit-tests-coverage
        fail_ci_if_error: false

  # =============================================================================
  # INTEGRATION TESTS
  # =============================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: ${{ !inputs.skip_tests }}
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"
    
    - name: Start test services
      run: |
        docker-compose -f tests/docker-compose.test.yml up -d
    
    - name: Wait for services to be ready
      run: |
        timeout 60 bash -c 'until docker-compose -f tests/docker-compose.test.yml ps | grep healthy; do sleep 2; done'
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ \
          --verbose \
          --tb=short \
          --junitxml=test-results-integration.xml \
          --timeout=300
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-integration
        path: test-results-integration.xml
    
    - name: Cleanup test services
      if: always()
      run: |
        docker-compose -f tests/docker-compose.test.yml down -v
        docker system prune -f

  # =============================================================================
  # END-TO-END TESTS
  # =============================================================================
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: ${{ !inputs.skip_tests && (github.event_name == 'push' || github.event.pull_request.draft == false) }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"
    
    - name: Build Docker images
      run: |
        docker build -t agent-mesh:test .
    
    - name: Run E2E tests
      run: |
        pytest tests/e2e/ \
          --verbose \
          --tb=short \
          --junitxml=test-results-e2e.xml \
          --timeout=600
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-e2e
        path: |
          test-results-e2e.xml
          tests/e2e/screenshots/
          tests/e2e/logs/
    
    - name: Upload container logs
      if: failure()
      run: |
        mkdir -p logs
        docker-compose -f tests/docker-compose.e2e.yml logs > logs/docker-compose.log
    
    - name: Upload logs
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: e2e-logs
        path: logs/

  # =============================================================================
  # BUILD VALIDATION
  # =============================================================================
  build-validation:
    name: Build Validation
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip build twine
        npm install
    
    - name: Build Python package
      run: |
        python -m build
        twine check dist/*
    
    - name: Build documentation
      run: |
        pip install -e ".[docs]"
        sphinx-build -b html docs/ docs/_build/html/
    
    - name: Build Docker image
      run: |
        docker build \
          --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
          --build-arg VCS_REF=${{ github.sha }} \
          --tag ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \
          .
    
    - name: Test Docker image
      run: |
        docker run --rm \
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \
          agent-mesh --version
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: build-artifacts
        path: |
          dist/
          docs/_build/html/
    
    - name: Upload Docker image (for further testing)
      if: github.event_name == 'push'
      run: |
        echo ${{ secrets.GITHUB_TOKEN }} | docker login ${{ env.REGISTRY }} -u ${{ github.actor }} --password-stdin
        docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}

  # =============================================================================
  # PERFORMANCE REGRESSION TESTS
  # =============================================================================
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: ${{ !inputs.skip_tests && github.event_name == 'push' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ \
          --verbose \
          --tb=short \
          --benchmark-only \
          --benchmark-json=benchmark-results.json
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark-results.json
    
    - name: Compare with baseline (if available)
      run: |
        if [ -f "baseline-benchmark.json" ]; then
          pytest-benchmark compare baseline-benchmark.json benchmark-results.json
        fi

  # =============================================================================
  # RESULTS AGGREGATION
  # =============================================================================
  test-results:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs: [code-quality, security-scan, unit-tests, integration-tests, e2e-tests, build-validation]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
    
    - name: Publish test results
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always()
      with:
        files: "**/test-results-*.xml"
        comment_mode: create new
        check_name: "Test Results Summary"
    
    - name: Generate summary report
      run: |
        echo "## CI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.code-quality.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Scan | ${{ needs.security-scan.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| E2E Tests | ${{ needs.e2e-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Build Validation | ${{ needs.build-validation.result }} |" >> $GITHUB_STEP_SUMMARY
    
    - name: Check overall status
      run: |
        if [[ "${{ needs.code-quality.result }}" != "success" || 
              "${{ needs.security-scan.result }}" != "success" || 
              "${{ needs.unit-tests.result }}" != "success" || 
              "${{ needs.integration-tests.result }}" != "success" || 
              "${{ needs.build-validation.result }}" != "success" ]]; then
          echo "❌ CI Pipeline failed - check individual job results"
          exit 1
        else
          echo "✅ CI Pipeline completed successfully"
        fi